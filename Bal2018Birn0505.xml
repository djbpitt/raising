<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="balisage-1-3.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<?xml-stylesheet type="text/xsl" href="balisage-proceedings-html.xsl"?>
<article xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink">
    <title>Flattening and unflattening XML markup</title>
    <info>
        <abstract>
            <para>From time to time, it may be necessary or expedient to flatten our XML documents
                by replacing the start- and end-tags of conventional XML content elements with empty
                place-marker elements (variously known as <emphasis role="ital">milestone
                    elements</emphasis> or as <emphasis>Trojan horse markup</emphasis>). When we do,
                we will often wish, later, to restore the content elements we flattened. The purpose
                of this late-breaking presentation is to present a survey of ways to perform the
                task of unflattening or of raising: restoring a conventional XML element structure
                of content elements from a flattened XML document instance (or part of one), and
                comparing different solutions to see what we can learn from them.</para>
        </abstract>
        <author>
            <personname>
                <firstname>David</firstname>
                <othername>J.</othername>
                <surname>Birnbaum</surname>
            </personname>
            <personblurb>
                <para>David J. Birnbaum is Professor and Co-Chair of the Department of Slavic
                    Languages and Literatures at the University of Pittsburgh. He has been involved
                    in the study of electronic text technology since the mid-1980s, has delivered
                    presentations at a variety of electronic text technology conferences, and has
                    served on the board of the Association for Computers and the Humanities, the
                    editorial board of <emphasis role="ital">Markup languages: theory and
                        practice</emphasis>, and the Text Encoding Initiative Council. Much of his
                    electronic text work intersects with his research in medieval Slavic manuscript
                    studies, but he also often writes about issues in the philosophy of
                    markup.</para>
            </personblurb>
            <affiliation>
                <jobtitle>Professor of Slavic Languages and Literatures</jobtitle>
                <orgname>University of Pittsburgh, Pittsburgh, PA</orgname>
            </affiliation>
            <email>ebb8@pitt.edu</email>
        </author>
        <author>
            <personname>
                <firstname>Elisa</firstname>
                <othername>Eileen</othername>
                <surname>Beshero-Bondar</surname>
            </personname>
            <personblurb>
                <para>A scholar of British Romanticism and hybrid literary genres, Dr.
                    Beshero-Bondar earned a PhD in English Literature from Penn State University in
                    2003, and afterwards took a post teaching literature at the University of
                    Pittsburgh at Greensburg. Her publications include a book about women Romantic
                    epoists, titled <emphasis role="ital">Women, epic, and transition in British
                        Romanticism</emphasis>, published by the University of Delaware Press in
                    2011, and articles in <emphasis role="ital">Literature compass</emphasis>,
                        <emphasis role="ital">ELH</emphasis> (English literary history), <emphasis
                        role="ital">Genre</emphasis>, and <emphasis role="ital">Philological
                        quarterly</emphasis> on the poetry of Robert Southey, Mary Russell Mitford,
                    and Lord Byron in context with 18th- and 19th-century views of revolution, world
                    empires, natural sciences, and theater productions.</para>
                <para>Since earning tenure in 2011, she has applied herself adventurously to the
                    building of digital editions and digital research projects, such as studying
                    studying associations among physical and mythical locations in epic poetry, and
                    teaching undergraduates the XML family of languages in the context of designing
                    research projects. She is the director of the <link
                        xlink:href="http://digitalmitford.org">Digital Mitford project</link>, whose
                    two-fold mission is:<orderedlist>
                        <listitem>
                            <para>to produce the first comprehensive scholarly edition of the works
                                and letters of Mary Russell Mitford, and</para>
                        </listitem>
                        <listitem>
                            <para>to share knowledge of TEI XML and other related humanities
                                computing practices with all serious scholars interested in
                                contributing to the project.</para>
                        </listitem>
                    </orderedlist> In keeping with the second goal, she hosts an annual coding
                    school at Pitt-Greensburg each May or June to teach TEI, regular expression
                    up-conversion of documents, XPath, schema design, and XSLT or XQuery, based on
                    the interests and background of registrants. Other digital projects in which she
                    is enmeshed include <link
                        xlink:href="https://github.com/ebeshero/Pittsburgh_Frankenstein/tree/Text_Processing"
                        >a Bicentennial Frankenstein project</link> to up-convert the 1990s
                    electronic Frankenstein edition by 2018, and <link
                        xlink:href="http://amadis.newtfire.org/">Amadis In Translation</link>, which
                    applies XML to quantify, categorize, and study alterations made by Robert
                    Southey in translating an early modern Spanish text of Amadis de Gaule. She was
                    elected to serve on <link xlink:href="http://www.tei-c.org/Activities/Council/"
                        >the TEI Technical Council</link> from 2016-2017.</para>
            </personblurb>
            <affiliation>
                <jobtitle>Associate Professor of English</jobtitle>
                <jobtitle>Director, <link
                        xlink:href="http://www.greensburg.pitt.edu/digital-humanities/center-digital-text"
                        >Center for the Digital Text</link></jobtitle>
                <orgname>University of Pittsburgh at Greensburg</orgname>
            </affiliation>
            <email>ebb8@pitt.edu</email>
        </author>
        <author>
            <personname>
                <firstname>C.</firstname>
                <othername>M.</othername>
                <surname>Sperberg-McQueen</surname>
            </personname>
            <personblurb>
                <para>C. M. Sperberg-McQueen is the founder and principal of <link
                        xlink:href="http://www.blackmesatech.com/">Black Mesa Technologies</link>, a
                    consultancy specializing in helping memory institutions improve the long term
                    preservation of and access to the information for which they are responsible. He
                    served as editor in chief of the TEI Guidelines from 1988 to 2000, and has also
                    served as co-editor of the World Wide Web Consortium's XML 1.0 and XML Schema
                    1.1 specifications.</para>
            </personblurb>
            <affiliation>
                <jobtitle>Founder and Principal</jobtitle>
                <orgname>Black Mesa Technologies</orgname>
            </affiliation>
            <email>cmsmcq@blackmesatech.com</email>
        </author>
    </info>
    <section>
        <title>Overview</title>
        <para>From time to time, it may be necessary or expedient to flatten our XML documents by
            replacing the start- and end-tags of conventional XML content elements with empty
            place-marker elements (variously known as <emphasis role="ital">milestone
                elements</emphasis>, after the milestone technique described in the TEI Guidelines
            for page beginnings, column beginnings, line beginnings, etc. [<xref linkend="tei_p5"
            />], or as <emphasis>Trojan horse markup</emphasis>, after the technique described by
            Steve DeRose [<xref linkend="derose_2004"/>]). When we do, we will often wish, later, to
            restore the content elements we flattened. The three co-authors discovered recently that
            we had each had occasion to perform this task, and that we had undertaken it using
            different techniques.</para>
        <para>The purpose of this late-breaking presentation is to present a survey of ways to
            perform the task of unflattening or of raising: restoring a conventional XML element
            structure of content elements from a flattened XML document instance (or part of one),
            and comparing different solutions to see what we can learn from them. Nothing here is
            profoundly difficult or new, but each of us found it challenging and interesting enough
            that we think it may be worth while to share what we have learned with others.</para>
        <para>In the following sections, we describe first a concrete instance of the task, with
            enough supporting detail to make clear that this is not an academic exercise, but one
            that arose in a concrete project. We then present several approaches to solving the
            problem, including some false starts, which illustrate possible wrong turnings along the
            way. We then discuss and compare the different solutions with respect to coding
            difficulty and costs in space and time.</para>
    </section>
    <section>
        <title>The concrete sample task (TODO: Elisa)</title>
        <para>As a concrete example, we can consider the form taken by this task in the Variorum
            Frankenstein [<xref linkend="frankenstein"/>] project edited by the second author. In
            order to collate different XML-encoded versions of the novel using the collation tool
            CollateX [<xref linkend="collatex"/>], it is necessary to work with individual word
            tokens, and to retain markup information in the eventual collation output without
            letting it interfere with the alignment process.</para>

        <para>We may, for example, wish to collate the following two fragments of <emphasis
                role="ital">Frankenstein</emphasis>:</para>

        <programlisting>[sample needed, first witness, in native TEI or other XML]</programlisting>

        <programlisting>[sample needed, second witness]</programlisting>
        <para>If we feed these documents to CollateX directly, the results are somewhat
            disappointing:</para>
        <programlisting>[sample output showing disastrous result]</programlisting>
        <para>To produce more useful results, we need the input to CollateX to take the following
            form for the first fragment:</para>
        <programlisting>[Flattened version of first fragment]</programlisting>
        <para>Similar changes are needed in the second fragment as well.</para>
        <para>Now the output from CollateX is more satisfactory:</para>
        <programlisting>[Sample output looking better]</programlisting>
        <para>In order to work normally with the output, however, we need to restore the original
            element structure.</para>
        <para>TODO: We can’t always restore the element structure because CollateX wraps each word
            token in tags. We might choose to merge some of these down the road, and we can raise
            those flattened elements that are located entirely within one merged sequence. But we
            may nonetheless wind up with an original start-tag inside one container element and its
            corresponding original end-tag inside another, and we cannot reconstruct that element
            without creating overlap. So: do we want, for example, to raise instances of
                <code>&lt;seg&gt;></code> that are inside a word token while not raising those that
            straddle word tokens? What is the cost of treating the same element type differently?
            For what it’s worth, in the <link xlink:href="http://pvl.obdurodon.org">PVL</link>, I
            flatten the markup for collation and then convert the milestones to symbols for
            rendering. That is, I don’t try to reconstruct the original hierarchy.</para>
    </section>
    <section>
        <title>Right-sibling traversal (TODO: Michael)</title>
        <para>[Description of the algorithm in the shallow-to-deep.xsl stylesheet on <link
                xlink:href="http://uyghur.ittc.ku.edu">shallow-to-deep.xsl</link>. Possibly two
            versions, for 1.0 and 3.0.]</para>
    </section>
    <section>
        <title>Inside-out processing: a recursive function (TODO: David)</title>
        <para>TODO: describe raising <emphasis role="ital">Frankenstein</emphasis> (current version
            is generic) and fix line-number references</para>
        <para>We turn off indentation (line 5) to avoid deforming the whitespace.
                <code>@exclude-result-prefixes="#all"</code> is not enough to avoid writing the
                <code>th:</code> namespace onto the root element of the output, even though the
            namespace in question is not used the output. An unused namespace declaration is
            informationally harmless, but also needlessly distracting, so we suppress it by spelling
            out the identity template (for all modes) and specifying
                <code>@copy-namespaces="no"</code> on <code>&lt;xsl:copy&gt;</code>inside it (lines
            6–10).</para>
        <para>Our recursive raising operation (the <code>th:raise()</code> function, lines 11–26)
            operates on document nodes, and we need to process the original document node of the
            input file differently from the new document nodes that we create on each pass through
            the recursive function. For that reason, we match the original document node in no mode
                (<code>&lt;xsl:template match="/"&gt;</code>, lines 27–29) and pass it into the
            raising function (<code>&lt;xsl:sequence select="th:raise(.)"/&gt;</code>, line
            28).</para>
        <para>The raising function checks for the presence of @th:sID attributes in the input
                (<code>&lt;xsl:when test="exists($input//@th:sID)"&gt;</code>, line 14). If there
            aren’t any (<code>&lt;xsl:otherwise&gt;</code>, lines 22–24), the recursion is finished,
            and the function returns the result (<code>&lt;xsl:sequence select="$input"/&gt;</code>,
            line 23). If there are still <code>@th:sID</code> attributes in the text, we create a
            variable <code>$result</code> (lines 15–19) of type document and apply templates inside
            the newly created document node (line 17). After the application of templates is
            finished, we recur and pass the result into another invocation of
                <code>th:raise()</code> (<code>&lt;xsl:sequence select="th:raise($result)"/></code>,
            line 20).</para>
        <para>The application of templates within the recursive function begins by applying
            templates to the (newly created) document node in loop mode
                (<code>&lt;xsl:apply-templates select="$input" mode="loop"/&gt;</code>, line 17).
            The matching template (lines 30–32) simply applies templates to its children, unlike the
            template that matches the original document node (in no mode, lines 27–29), which passes
            the document into the th:raise() function (line 28), a difference in mode that is needed
            to avoid an endless loop. All other processing is the same for both the original
            document and the interim documents created inside <code>th:raise()</code>, so
                <code>&lt;xsl:template match="/" mode="loop"&gt;</code> (lines 30–32) is the only
            modal template, and it applies templates to its children in no mode.</para>
        <para>There are three templates that do the actual processing of the innermost elements to
            be raised on each recursion: one that processes the start-tag, one that processes the
            content of the newly raised element, and one that processes the corresponding end-tag:<itemizedlist>
                <listitem>
                    <para><emphasis role="bold">start-tag:</emphasis> We match elements with an
                        @th:sID attribute that has a value equal to the value of a @th:eID attribute
                        on their first following sibling element that has a @th:eID attribute (line
                        34). This, then, matches only start-tags that contain nothing but
                            <code>text()</code> nodes and elements that have already been raised
                        (from which attributes in the <code>th:</code> namespacethat were present in
                        the input have been discarded). In other words, it matches only the
                        innermost flattened elements, those that do not contain any other empty
                        flattened elements. We process these hits by creating a container element
                        with the same generic identifier as the start-tag and copying all
                        following-sibling nodes that precede the end-tag that matches the start-tag
                        we’re processing at the moment (lines 36–41). In other words, we copy the
                        content of the newly raised element into it.</para>
                </listitem>
                <listitem>
                    <para><emphasis role="bold">nodes inside the new wrapper:</emphasis> We have
                        already copied the content of the newly raised element inside it, which
                        means that we don’t want to process those nodes again, since that would
                        create duplicates. For that reason, we match all nodes between the start-
                        and end-tags that we’re processing at the moment and suppress them by
                        matching them inside an empty <code>&lt;xsl:template&gt;</code> element
                        (lines 44–45).</para>
                </listitem>
                <listitem>
                    <para><emphasis role="bold">end-tag:</emphasis> Since we create real start- and
                        end-tags when we match the flattened start-tag, we have no more use for the
                        flattened end-tag, so we suppress it by matching it, too, inside an empty
                            <code>&lt;xsl:template&gt;</code> element (lines 47–48).</para>
                </listitem>
            </itemizedlist></para>
        <para>TODO: rewrite using a recursive named-template call for 1.0 with result element
            extension</para>
    </section>
    <section>
        <title>An XSLT 3.0 solution using accumulators (TODO: Michael)</title>
        <para>[To be written. I am assuming that we can find a way to use accumulators to handle
            this.]</para>
    </section>
    <section>
        <title>Global search and replace using regular expressions (TODO: David)</title>
        <para>It is not possible to parse arbitrary XML with regular expressions (regex), but it is
            not difficult to identify and process Trojan milestones.<footnote>
                <para>In terms of the Chomsky hierarchy, XML is a Type 2 (context-free) grammar and
                    regular expressions are a Type 3 (regular) grammar. Parsing a context-free
                    grammar, which permits recursion, requires a stack, which is not available in
                    regular grammars. While some modern regular expression implementations support
                    back-referencing and recursion and thus go beyond the limitations of a Chomsky
                    regular grammar, whether methods that rely on these extensions should be
                    considered regular-expression parsing is unclear (this is a theoretical issue),
                    as is the role of legibility in evaluating the suitability of the method to the
                    task (this is a practical concern).</para>
            </footnote></para>
        <para>The following regex matches Trojan start-tags:</para>
        <programlisting>(&lt;[^&gt;]+?)th:sID\s*?=\s*['"]\w+?['"](.*?)\/(&gt;)</programlisting>
        <para>The regex works as follows (in multiline mode, that is, where dot also matches
                <code>\n</code>):<itemizedlist>
                <listitem>
                    <para>The first capture group matches everything from the beginning of a tag
                        that contains a <code>@th:sID</code> attribute until that attribute name.
                        This necessarilly includes the space that precedes the attribute name, as
                        well as any attributes that might also precede it.</para>
                </listitem>
                <listitem>
                    <para>We do not capture any part of the <code>th:sID</code> attribute: the
                        attribute name, the equal sign (with optional whitespace before or after),
                        the quotation mark value delimiter (single or double), the attribute value
                        (all characters up to the closing value delimiter), and the closing value
                        delimiter. As long as the <code>th:sID</code> and <code>th:eID</code> values
                        are created with the XPath <code>generate-id()</code> function, they cannot
                        contain single or double quotation marks (<code>generate-id()</code> creates
                        only values that are XML names), so we do not need to verify that the
                        opening and closing delimiters match each other lexically.<footnote>
                            <para>“The returned identifier must consist of ASCII alphanumeric
                                characters and must start with an alphabetic character. Thus, the
                                string is syntactically an XML name.” [<xref linkend="xpath"/>,
                                    <link
                                    xlink:href="https://www.w3.org/TR/xpath-functions-31/#func-generate-id"
                                    >§14.5.4</link>]</para>
                        </footnote></para>
                </listitem>
                <listitem>
                    <para>The second capture group captures everything following the
                            <code>@th:sID</code> attribute up to the <code>/&gt;</code> that marks
                        the end of the tag.</para>
                </listitem>
                <listitem>
                    <para>We do not capture the <code>/</code> before the closing
                        <code>&gt;</code>.</para>
                </listitem>
                <listitem>
                    <para>The third capture group captures the closing <code>&gt;</code>.</para>
                </listitem>
            </itemizedlist></para>
        <para>We replace all matches with the following replacement pattern:</para>
        <programlisting>\1\2\3</programlisting>
        <para>The regex to match Trojan end-tags is similar to the one for start-tags, and because
            real end-tags cannot contain attributes, we do not need to match or copy them. We
            capture the opening <code>&lt;</code> separately from whatever follows it, so that we
            can write a <code>/</code> into the replacement after it. The regex is:</para>
        <programlisting>(&lt;)(\S+?)\s+[^&gt;]*?th:eID=['"]\w+['"][^&gt;]*?/(&gt;)</programlisting>
        <para>and the replacement pattern is:</para>
        <programlisting>\1/\2\3</programlisting>
        <para>This method will incorrectly apply the replacement to matching patterns within XML
            comments and CDATA marked sections. With apologies for disappointing Regex Edge-Case
            Bounty Hunters, coping with matches in these contexts, which would not naturally appear
            in our data, is not a goal in our work.<footnote>
                <para>Similarly, we rely on the use of <code>th:</code> as the namespace prefix and
                        <code>th:sID</code> and <code>th:eID</code> as the attribute names for our
                    Trojan attributes. Changing these assumptions is not a problem as long as the
                    regex is changed to match.</para>
            </footnote></para>
        <para>[In order to time this, we will want to perform these tasks with sed, emacs in batch
            mode, and/or Perl or Python.]</para>
    </section>
    <section>
        <title>Some things that can go wrong</title>
        <para>TODO: Michael: MSM’s bugs include a failure to specify the correct mode on an
            apply-templates call, which meant that processing slipped out of the special mode and
            into default mode, and many nodes appeared repeatedly in the input: tenfold increase in
            size of document, and more.</para>
        <para>TODO: Elisa: EBB’s initial sketch, with discussion of what is going wrong.</para>
        <para>TODO: Elisa: EBB’s implementation of right-sibling recursion, with analysis and
            discussion.</para>
        <para>TODO: David: DJB’s bugs, if any ...]</para>
    </section>
    <section>
        <title>Comparison (David)</title>
        <para>The inside-out recursive approach (both the function-based XSLT 3.0 version and
            named-template-based XSLT 1.0 one) is tail-recursive, which means that an XSLT processor
            that performs tail-call optimization will not be at risk for running out of stack space.
            In cases where tail-call optimization is not available, the maximum depth of recursion
            is equal to the <emphasis role="ital">depth</emphasis> of the deepest Trojan element in
            the input hierarchy. The right-sibling traversal approach (both the XSLT 3.0 and the
            XSLT 1.0 versions) is also tail-recursive (TODO: Michael, is this correct?), and with an
            XSLT parser that performs tail-call optimization, it requires stack space equal to the
            maximum <emphasis role="ital">width</emphasis> of the widest hierarchical level.</para>
        <para>Insofar as an open-source XSLT 3.0 processor that performs tail-call optimization is
            freely available in the open-source, platform-independent Saxon-HE product [<xref
                linkend="saxon"/>], the difference in stack requirements between the two methods has
            not been a consideration for our purposes. But insofar as XML documents of the sort that
            are of interest to digital humanists are typically wider than they are deep, users who
            are unable to employ an XSLT processor that performs tail-call optimization may favor
            inside-out processing over right-sibling traversal because inside-out processing is
            likely to require less stack space.</para>
        <para>(TODO: Michael: I find the inside-out approach simpler, that is, easier to understand,
            but I don’t know whether that’s because it is simpler, or because of the greater
            familiarity that comes with having developed it. If it is objectively simpler, should we
            mention that in our comparison?)</para>
        <para>TODO: Unassigned: [Measurement of time and space complexity running all solutions on
            the same data (as large as we can manage—e.g., all of <emphasis role="ital"
                >Frankenstein</emphasis>, or an artificial document containing 10 or 100 copies of
                <emphasis role="ital">Frankenstein</emphasis>).]</para>
        <para>TODO: Unassigned: [Time complexity is easy enough to measure; space complexity may be
            harder, but we may be able to find ways. I believe that the Unix tool <emphasis
                role="bold">top</emphasis> gives the current memory usage of a task, so the
            information is certainly available.]</para>
        <para>TODO Unassigned: [One reason I would like to have multiple XSLT 1.0 solutions is so
            that we can test performance in more than one implementation: <emphasis role="bold"
                >xsltproc</emphasis>, <emphasis role="bold">Xalan</emphasis>, and browser XSLT
            engines, if we can figure out how to persuade a browser to do the work.</para>
        <para>TODO Unassigned: [My prediction a priori is that using <emphasis role="bold"
                >sed</emphasis>, <emphasis role="bold">emacs</emphasis>, <emphasis role="bold"
                >perl</emphasis>, or <emphasis role="bold">Python</emphasis> to make the changes
            with regular expressions will be much faster than using XSLT. Without XSLT, it will be
            much easier to produce output that’s not well-formed; it would be good to document how
            many attempts are needed before our sed script is producing well-formed output, and how
            many more are needed before it’s both well formed and correct. djb: I spent about an
            hour crafting and debugging the regex expressions for the start and end tags. It
            produces well-formed output with my baby sample, recreating the original document
            (before flattening) faithfully. I look forward to the Frankentest!]</para>
    </section>
    <section>
        <title>Conclusion (TODO: Unassigned)</title>
        <para>[what do we say here? djb: The conclusion of my micro-pipelining paper last year may
            be applicable here, too. It reads: “None of the methods described here is new, but their
            explicit juxtaposition, comparison, and evaluation in a tutorial context based on real
            use cases has clarified much about micropipelining for the author, and, it is hoped, for
            the reader, as well.”]</para>
    </section>
    <bibliography>
        <title>Works cited</title>
        <bibliomixed xml:id="collatex" xreflabel="CollateX"><emphasis role="ital">CollateX: software
                for collating textual sources.</emphasis>
            <link>https://collatex.net/-</link></bibliomixed>
        <bibliomixed xml:id="derose_2004" xreflabel="DeRose 2004">DeRose, Steve. 2004. “Markup
            Overlap: a review and a horse.” Presented at Extreme Markup Languages 2004. Montréal,
            Québec, August 2-6, 2004.
            <link>http://xml.coverpages.org/DeRoseEML2004.pdf</link></bibliomixed>
        <bibliomixed xml:id="tei_p5" xreflabel="TEI P5">P5: <emphasis role="ital">Guidelines for
                electronic text encoding and interchange.</emphasis>
            <link>http://www.tei-c.org/guidelines/P5/</link></bibliomixed>
        <bibliomixed xml:id="frankenstein" xreflabel="Variorum Frankenstein"><emphasis role="ital"
                >Pittsburgh</emphasis> Frankenstein <emphasis role="ital">Project.</emphasis>
            <link>https://github.com/PghFrankenstein/Pittsburgh_Frankenstein</link></bibliomixed>
        <bibliomixed xml:id="saxon" xreflabel="Saxon-HE">Saxon-HE (home edition).
                <link>http://saxon.sourceforge.net/</link></bibliomixed>
        <bibliomixed xml:id="xpath" xreflabel="XPath functions"><emphasis role="ital">XPath and
                XQuery functions and operators 3.1 W3C recommendation 21 March 2017.</emphasis>
            <link>https://www.w3.org/TR/xpath-functions-31/</link></bibliomixed>
    </bibliography>
</article>
